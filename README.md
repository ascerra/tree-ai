# tree-ai ğŸ§ ğŸŒ²

`tree-ai` is a command-line tool that enhances the `tree` output with AI-generated descriptions of each file and folder using IBM's Granite 4.0 Tiny Preview model. It works entirely offline after install, and outputs AI-enriched directory listings.

---

## ğŸš€ Install and Run

### ğŸ” Option 1: One-liner install and run (via `curl`)

```bash
curl -sSf https://raw.githubusercontent.com/ascerra/tree-ai/main/install-and-run.sh | bash -s ./my-project
```

This will:
- Clone the repo into a temp directory
- Set up a local Python virtual environment
- Download the Granite model via Hugging Face
- Build the Go CLI and AI runner
- Run `tree-ai` on the specified directory

---


### ğŸ›  Option 2: Manual install for development

```bash
# Step 1: Clone the repo
git clone https://github.com/ascerra/tree-ai.git

# Step 2: Move into the project directory
cd tree-ai

# Step 3: Build and install dependencies
make install

# Step 4: Activate your Python virtual environment
source .venv/bin/activate

# Step 5: Run tree-ai on a directory
./bin/tree-ai ./my-project
```

---

## ğŸ’¡ What You Get

```bash
./
â”œâ”€â”€ cmd (CLI entrypoint)
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ ai/ (AI integration using IBM Granite)
â”‚   â””â”€â”€ tree/ (tree display logic)
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ granite-runner.go (Go CLI wrapper around Granite model)
â”‚   â””â”€â”€ granite_infer.py (Python script running IBM Granite inference)
```

When you run `tree-ai`, youâ€™ll see output like:

```
.
â”œâ”€â”€ main.go (Handles CLI setup and command execution)
â”œâ”€â”€ internal (Contains logic for AI and tree display)
â”‚   â””â”€â”€ ai (Integrates IBM Granite model for description generation)
```

---

## ğŸ“¦ Project Structure

- `main.go` â€“ entrypoint, calls `cmd.Execute()`
- `cmd/` â€“ Cobra CLI setup
- `internal/` â€“ Go logic for tree rendering and AI prompt integration
- `model/` â€“ AI runner files (`granite-runner.go` and `granite_infer.py`)
- `.venv/` â€“ Python virtualenv created by `make install`

---

## ğŸ›  Requirements

- Go 1.20+
- Python 3.8+
- Internet access for first-time model download via Hugging Face

---

## ğŸ” Offline by Default

Once installed, all AI inference runs locally using the IBM Granite 4.0 Tiny Preview model. No data is sent to any server after setup.

---

## ğŸ§¹ Optional Cleanup

```bash
make clean
rm -rf .venv/
```

---

## ğŸ§ª Testing

```bash
make test
make cover  # to see test coverage
```

---

## âœ¨ Coming Soon

- Shell completion
- Model choice via flags (e.g. --model)
- Built-in file filters and summarization toggles